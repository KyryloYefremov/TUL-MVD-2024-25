{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 9. cvičení\n",
    "\n",
    "Dnešní cvičení nebude až tak obtížné. Cílem je seznámit se s HuggingFace a vyzkoušet si základní práci s BERT modelem.\n",
    "\n",
    "## 1. část - Seznámení s HuggingFace a modelem BERT\n",
    "\n",
    "Nainstalujte si Python knihovnu `transformers` a podívejte se na předtrénovaný [BERT model](https://huggingface.co/bert-base-uncased). Vyzkoušejte si unmasker s různými vstupy.\n",
    "\n",
    "<br>\n",
    "Pozn.: Použití BERT modelu vyžaduje zároveň PyTorch - postačí i cpu verze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- INPUT 0 ----\n",
      "Option: the first president of the usd was george smith. (Score: 0.0137)\n",
      "Option: the first president of the usd was george johnson. (Score: 0.0103)\n",
      "Option: the first president of the usd was george white. (Score: 0.0103)\n",
      "Option: the first president of the usd was george brown. (Score: 0.0102)\n",
      "Option: the first president of the usd was george williams. (Score: 0.0094)\n",
      "\n",
      "---- INPUT 1 ----\n",
      "Option: the capital of france is paris. (Score: 0.4168)\n",
      "Option: the capital of france is lille. (Score: 0.0714)\n",
      "Option: the capital of france is lyon. (Score: 0.0634)\n",
      "Option: the capital of france is marseille. (Score: 0.0444)\n",
      "Option: the capital of france is tours. (Score: 0.0303)\n",
      "\n",
      "---- INPUT 2 ----\n",
      "Option: two plus two equals one. (Score: 0.3128)\n",
      "Option: two plus two equals three. (Score: 0.2023)\n",
      "Option: two plus two equals two. (Score: 0.1390)\n",
      "Option: two plus two equals four. (Score: 0.0606)\n",
      "Option: two plus two equals zero. (Score: 0.0582)\n",
      "\n",
      "---- INPUT 3 ----\n",
      "Option: the kenobi was a jedi master and anakin skywalker ' s teacher. (Score: 0.3479)\n",
      "Option: master kenobi was a jedi master and anakin skywalker ' s teacher. (Score: 0.1970)\n",
      "Option: john kenobi was a jedi master and anakin skywalker ' s teacher. (Score: 0.0244)\n",
      "Option: jedi kenobi was a jedi master and anakin skywalker ' s teacher. (Score: 0.0227)\n",
      "Option: professor kenobi was a jedi master and anakin skywalker ' s teacher. (Score: 0.0162)\n"
     ]
    }
   ],
   "source": [
    "# read unmasking pipeline\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "mask = '[MASK]'\n",
    "inputs = [\n",
    "    f'The first president of the USD was George {mask}.',\n",
    "    f'The capital of France is {mask}.',\n",
    "    f'Two plus two equals {mask}.',\n",
    "    f\"{mask} Kenobi was a Jedi Master and Anakin Skywalker's teacher.\",\n",
    "]\n",
    "\n",
    "for i, input in enumerate(inputs):\n",
    "    print(f\"\\n---- INPUT {i} ----\")\n",
    "    outputs = unmasker(input)\n",
    "    for output in outputs:\n",
    "        print(f\"Option: {output['sequence']} (Score: {output['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. část - BERT contextualized word embeddings\n",
    "\n",
    "BERT dokumentace obsahuje také návod jak použít tento model pro získání word embeddingů. Vyzkoušejte použití stejného slova v různém kontextu a podívejte se, jak se mění kosinova podobnost embeddingů v závislosti na kontextu daného slova.\n",
    "\n",
    "Podívejte se na výstup tokenizeru před vstupem do BERT modelu - kolik tokenů bylo vytvořeno pro větu \"Hello, this is Bert.\"? Zdůvodněte jejich počet.\n",
    "\n",
    "<br>\n",
    "Pozn.: Vyřešení předchozí otázky Vám pomůže zjistit, který vektor z výstupu pro cílové slovo použít."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', ',', 'this', 'is', 'bert', '.']\n",
      "Number of tokens: 6\n"
     ]
    }
   ],
   "source": [
    "# read tokenizeru\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# tokenize the sentence\n",
    "sentence = \"Hello, this is Bert.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "input_ids = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between `key` in two contexts: 0.4941\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence1 = \"Knowledge - is the key to the Universe.\"\n",
    "sentence2 = \"I lost the key to my car.\"\n",
    "\n",
    "word = 'key'\n",
    "\n",
    "# tokenize\n",
    "inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "# calculate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# select embeddings for our `word` \n",
    "# find index of the token `word` in every sentence\n",
    "index1 = tokenizer.convert_ids_to_tokens(inputs1[\"input_ids\"][0]).index(word)\n",
    "index2 = tokenizer.convert_ids_to_tokens(inputs2[\"input_ids\"][0]).index(word)\n",
    "\n",
    "# get embeddings for token\n",
    "embedding1 = outputs1.last_hidden_state[0, index1, :].numpy()\n",
    "embedding2 = outputs2.last_hidden_state[0, index2, :].numpy()\n",
    "\n",
    "similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "print(f\"Cosine similarity between `{word}` in two contexts: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Vizualizace slovních  embeddingů\n",
    "\n",
    "Vizualizujte slovní embeddingy - mění se jejich pozice v závislosti na kontextu tak, jak byste očekávali? Pokuste se vizualizovat i některá slova, ke kterým by se podle vás cílové slovo mělo po změně kontextu přiblížit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
