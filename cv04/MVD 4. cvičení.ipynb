{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVD 4. cvičení\n",
    "\n",
    "## 1. část - Načtení dat\n",
    "\n",
    "Po rozbalení archive.zip uvidíte articles csv soubor. Tento soubor pochází z [Kaggle datasetů](https://www.kaggle.com/hsankesara/medium-articles) a obsahuje malé množství Medium článků k tématům ML, AI a data science. K úloze dnešního cvičení bude stačit využítí dat s názvy a obsahy článků (title a text).\n",
    "\n",
    "\n",
    "### Příprava dat\n",
    "\n",
    "Pro přípravu dat se použivá různá sekvence kroků. Je doporučeno na následující kroky vytvořit samostatnou funkci, aby bylo možné zpracovat i vyhledávaný výraz při testování. Dnešní cvičení by mělo obsahovat následující kroky:\n",
    "\n",
    "1. Převést všechen text na lower case\n",
    "2. Odstranění interpunkce a všech speciálních znaků (apostrof, ...)\n",
    "3. Aplikace lemmatizátoru\n",
    "\n",
    "Pozn.: Jedná se pouze o jednoduchý preprocessing, v praxi je často potřeba použití více kroků. Tato aplikace by měla například problém s čísly (desetinná čísla, čísla vyhledávaná slovně). \n",
    "\n",
    "Pro lemmatizaci použijte knihovnu spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T20:19:19.653435Z",
     "start_time": "2024-10-09T20:18:35.164699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instalace spaCy z Jupyter Notebooku\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "\n",
    "# Stažení modelu pro angličtinu\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T08:30:48.364409Z",
     "start_time": "2024-10-11T08:30:46.675428Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import csv\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # NLTK\n",
    "# Lemmatizace textu př.:  \n",
    "# \" \".join([token.lemma_ for token in lemmatizer(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkce pro preprocessing textu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefg hig k l m. n10 t... xy{z}\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "# Simple test\n",
    "print(to_lowercase(\"AbCdEfG HiG K L M. N10 T... xy{Z}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my name 1is optimus prime\n"
     ]
    }
   ],
   "source": [
    "def remove_spec_chars(text: str) -> str:\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Simple test\n",
    "print(remove_spec_chars(\"h@ello, m$y name   1is optimus p-r.i+m!e'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to be love by yours heart\n"
     ]
    }
   ],
   "source": [
    "def apply_lemmatizer(text: str, lemmatizer) -> list[str]:\n",
    "    return \" \".join([token.lemma_ for token in lemmatizer(text)])\n",
    "\n",
    "print(apply_lemmatizer(\"I wanted to be loved by yours heart\", lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab123 asdasd asd\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text: str) -> list[str]:\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_spec_chars(text)\n",
    "    text = apply_lemmatizer(text, lemmatizer=lemmatizer)\n",
    "    return text\n",
    "\n",
    "print(preprocess('Ab-!@#123 asdasd asd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Načítání članků z .csv souboru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author', 'claps', 'reading_time', 'link', 'title', 'text']\n",
      "\n",
      "Chatbots were the next big thing: what happened? – The Startup – Medium\n",
      "\n",
      "Oh, how the headlines blared:\n",
      "Chatbots were The Next Big Thing.\n",
      "Our hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start socializing with machines.\n",
      "And why wouldn’t they be? All the road signs pointed towards insane success.\n",
      "At the Mobile World Congress 2017, chatbots were the main headliners. The conference organizers cited an ‘overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots’.\n",
      "In fact, the only significant question around chatbots was who would monopolize the field, not whether chatbots would take off in the first place:\n",
      "One year on, we have an answer to that question.\n",
      "No.\n",
      "Because there isn’t even an ecosystem for a platform to dominate.\n",
      "Chatbots weren’t the first technological development to be talked up in grandiose terms and then slump spectacularly.\n",
      "The age-old hype cycle unfolded in familiar fashion...\n",
      "Expectations built, built, and then..... It all kind of fizzled out.\n",
      "The predicted paradim shift didn’t materialize.\n",
      "And apps are, tellingly, still alive and well.\n",
      "We look back at our breathless optimism and turn to each other, slightly baffled:\n",
      "“is that it? THAT was the chatbot revolution we were promised?”\n",
      "Digit’s Ethan Bloch sums up the general consensus:\n",
      "According to Dave Feldman, Vice President of Product Design at Heap, chatbots didn’t just take on one difficult problem and fail: they took on several and failed all of them.\n",
      "Bots can interface with users in different ways. The big divide is text vs. speech. In the beginning (of computer interfaces) was the (written) word.\n",
      "Users had to type commands manually into a machine to get anything done.\n",
      "Then, graphical user interfaces (GUIs) came along and saved the day. We became entranced by windows, mouse clicks, icons. And hey, we eventually got color, too!\n",
      "Meanwhile, a bunch of research scientists were busily developing natural language (NL) interfaces to databases, instead of having to learn an arcane database query language.\n",
      "Another bunch of scientists were developing speech-processing software so that you could just speak to your computer, rather than having to type. This turned out to be a whole lot more difficult than anyone originally realised:\n",
      "The next item on the agenda was holding a two-way dialog with a machine. Here’s an example dialog (dating back to the 1990s) with VCR setup system:\n",
      "Pretty cool, right? The system takes turns in collaborative way, and does a smart job of figuring out what the user wants.\n",
      "It was carefully crafted to deal with conversations involving VCRs, and could only operate within strict limitations.\n",
      "Modern day bots, whether they use typed or spoken input, have to face all these challenges, but also work in an efficient and scalable way on a variety of platforms.\n",
      "Basically, we’re still trying to achieve the same innovations we were 30 years ago.\n",
      "Here’s where I think we’re going wrong:\n",
      "An oversized assumption has been that apps are ‘over’, and would be replaced by bots.\n",
      "By pitting two such disparate concepts against one another (instead of seeing them as separate entities designed to serve different purposes) we discouraged bot development.\n",
      "You might remember a similar war cry when apps first came onto the scene ten years ago: but do you remember when apps replaced the internet?\n",
      "It’s said that a new product or service needs to be two of the following: better, cheaper, or faster. Are chatbots cheaper or faster than apps? No — not yet, at least.\n",
      "Whether they’re ‘better’ is subjective, but I think it’s fair to say that today’s best bot isn’t comparable to today’s best app.\n",
      "Plus, nobody thinks that using Lyft is too complicated, or that it’s too hard to order food or buy a dress on an app. What is too complicated is trying to complete these tasks with a bot — and having the bot fail.\n",
      "A great bot can be about as useful as an average app. When it comes to rich, sophisticated, multi-layered apps, there’s no competition.\n",
      "That’s because machines let us access vast and complex information systems, and the early graphical information systems were a revolutionary leap forward in helping us locate those systems.\n",
      "Modern-day apps benefit from decades of research and experimentation. Why would we throw this away?\n",
      "But, if we swap the word ‘replace’ with ‘extend’, things get much more interesting.\n",
      "Today’s most successful bot experiences take a hybrid approach, incorporating chat into a broader strategy that encompasses more traditional elements.\n",
      "The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response.\n",
      "Another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these.\n",
      "For plenty of companies, bots just aren’t the right solution. The past two years are littered with cases of bots being blindly applied to problems where they aren’t needed.\n",
      "Building a bot for the sake of it, letting it loose and hoping for the best will never end well:\n",
      "The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input.\n",
      "The advantage of this approach is that it’s pretty easy to list all the cases that they are designed to cover. And that’s precisely their disadvantage, too.\n",
      "That’s because these bots are purely a reflection of the capability, fastidiousness and patience of the person who created them; and how many user needs and inputs they were able to anticipate.\n",
      "Problems arise when life refuses to fit into those boxes.\n",
      "According to recent reports, 70% of the 100,000+ bots on Facebook Messenger are failing to fulfil simple user requests. This is partly a result of developers failing to narrow their bot down to one strong area of focus.\n",
      "When we were building GrowthBot, we decided to make it specific to sales and marketers: not an ‘all-rounder’, despite the temptation to get overexcited about potential capabilties.\n",
      "Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly.\n",
      "A competent developer can build a basic bot in minutes — but one that can hold a conversation? That’s another story. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like.\n",
      "In an ideal world, the technology known as NLP (natural language processing) should allow a chatbot to understand the messages it receives. But NLP is only just emerging from research labs and is very much in its infancy.\n",
      "Some platforms provide a bit of NLP, but even the best is at toddler-level capacity (for example, think about Siri understanding your words, but not their meaning.)\n",
      "As Matt Asay outlines, this results in another issue: failure to capture the attention and creativity of developers.\n",
      "And conversations are complex. They’re not linear. Topics spin around each other, take random turns, restart or abruptly finish.\n",
      "Today’s rule-based dialogue systems are too brittle to deal with this kind of unpredictability, and statistical approaches using machine learning are just as limited. The level of AI required for human-like conversation just isn’t available yet.\n",
      "And in the meantime, there are few high-quality examples of trailblazing bots to lead the way. As Dave Feldman remarked:\n",
      "Once upon a time, the only way to interact with computers was by typing arcane commands to the terminal. Visual interfaces using windows, icons or a mouse were a revolution in how we manipulate information\n",
      "There’s a reasons computing moved from text-based to graphical user interfaces (GUIs). On the input side, it’s easier and faster to click than it is to type.\n",
      "Tapping or selecting is obviously preferable to typing out a whole sentence, even with predictive (often error-prone ) text. On the output side, the old adage that a picture is worth a thousand words is usually true.\n",
      "We love optical displays of information because we are highly visual creatures. It’s no accident that kids love touch screens. The pioneers who dreamt up graphical interface were inspired by cognitive psychology, the study of how the brain deals with communication.\n",
      "Conversational UIs are meant to replicate the way humans prefer to communicate, but they end up requiring extra cognitive effort. Essentially, we’re swapping something simple for a more-complex alternative.\n",
      "Sure, there are some concepts that we can only express using language (“show me all the ways of getting to a museum that give me 2000 steps but don’t take longer than 35 minutes”), but most tasks can be carried out more efficiently and intuitively with GUIs than with a conversational UI.\n",
      "Aiming for a human dimension in business interactions makes sense.\n",
      "If there’s one thing that’s broken about sales and marketing, it’s the lack of humanity: brands hide behind ticket numbers, feedback forms, do-not-reply-emails, automated responses and gated ‘contact us’ forms.\n",
      "Facebook’s goal is that their bots should pass the so-called Turing Test, meaning you can’t tell whether you are talking to a bot or a human. But a bot isn’t the same as a human. It never will be.\n",
      "A conversation encompasses so much more than just text.\n",
      "Humans can read between the lines, leverage contextual information and understand double layers like sarcasm. Bots quickly forget what they’re talking about, meaning it’s a bit like conversing with someone who has little or no short-term memory.\n",
      "As HubSpot team pinpointed:\n",
      "People aren’t easily fooled, and pretending a bot is a human is guaranteed to diminish returns (not to mention the fact that you’re lying to your users).\n",
      "And even those rare bots that are powered by state-of-the-art NLP, and excel at processing and producing content, will fall short in comparison.\n",
      "And here’s the other thing. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans.\n",
      "But is that how humans prefer to interact with machines?\n",
      "Not necessarily.\n",
      "At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure.\n",
      "In a way, those early-adopters weren’t entirely wrong.\n",
      "People are yelling at Google Home to play their favorite song, ordering pizza from the Domino’s bot and getting makeup tips from Sephora. But in terms of consumer response and developer involvement, chatbots haven’t lived up to the hype generated circa 2015/16.\n",
      "Not even close.\n",
      "Computers are good at being computers. Searching for data, crunching numbers, analyzing opinions and condensing that information.\n",
      "Computers aren’t good at understanding human emotion. The state of NLP means they still don’t ‘get’ what we’re asking them, never mind how we feel.\n",
      "That’s why it’s still impossible to imagine effective customer support, sales or marketing without the essential human touch: empathy and emotional intelligence.\n",
      "For now, bots can continue to help us with automated, repetitive, low-level tasks and queries; as cogs in a larger, more complex system. And we did them, and ourselves, a disservice by expecting so much, so soon.\n",
      "But that’s not the whole story.\n",
      "Yes, our industry massively overestimated the initial impact chatbots would have. Emphasis on initial.\n",
      "As Bill Gates once said:\n",
      "The hype is over. And that’s a good thing. Now, we can start examining the middle-grounded grey area, instead of the hyper-inflated, frantic black and white zone.\n",
      "I believe we’re at the very beginning of explosive growth. This sense of anti-climax is completely normal for transformational technology.\n",
      "Messaging will continue to gain traction. Chatbots aren’t going away. NLP and AI are becoming more sophisticated every day.\n",
      "Developers, apps and platforms will continue to experiment with, and heavily invest in, conversational marketing.\n",
      "And I can’t wait to see what happens next.\n",
      "From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\n",
      "Head of Growth for GrowthBot, Messaging & Conversational Strategy @HubSpot\n",
      "Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi\n",
      "\n",
      "Titles number: 337, Texts number: 337\n"
     ]
    }
   ],
   "source": [
    "csv_filename = \"articles.csv\"\n",
    "titles = []\n",
    "texts = []\n",
    "\n",
    "with open(csv_filename, 'r') as file:\n",
    "    articles = csv.reader(file)\n",
    "    print(next(articles.__iter__()), end=\"\\n\\n\")  # Printing the columns names\n",
    "    \n",
    "    for line in articles:\n",
    "        titles.append(line[-2])\n",
    "        texts.append(line[-1])\n",
    "\n",
    "print(titles[0], end=\"\\n\\n\")\n",
    "print(texts[0])\n",
    "print(f\"Titles number: {len(titles)}, Texts number: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. část - Vytvoření invertovaného indexu\n",
    "\n",
    "Před další prací s textem je potřeba vytvořit invertovaný index, který poté usnadní práci. Invertovaný index bude slovník, kde klíčem bude slovo a hodnotou bude list s id dokumentů (index), které dané slovo obsahují.\n",
    "\n",
    "Pozn.: Je potřeba vytvořit dva invertované indexy - jeden pro title a druhý pro text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(doc_str_list: list[str]) -> defaultdict[list]:\n",
    "    \"\"\"\n",
    "    Creates a dict, where:\n",
    "        key: str -  word from text;\n",
    "        value: list[int] - list of documents ids, where this word was located.\n",
    "    Args:\n",
    "        doc_str_list: list[str] - list of string documents texts\n",
    "    Return:\n",
    "        inv_idx_dict: defaultdict[list] - inverted indices dictionary, where there are all words from from all documents\n",
    "    \"\"\"\n",
    "    inv_idx_dict = defaultdict(list)\n",
    "\n",
    "    for doc_id, text in enumerate(doc_str_list):\n",
    "\n",
    "        # Per word: add doc id to list of word ids if it isn't in list already\n",
    "        for doc_w in text.split():\n",
    "            if doc_id not in inv_idx_dict.get(doc_w, []):\n",
    "                inv_idx_dict[doc_w].append(doc_id)\n",
    "\n",
    "    return inv_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess titles and texts of each document\n",
    "prep_titles = [preprocess(title) for title in titles]\n",
    "prep_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "titles_inv_idx = create_inverted_index(prep_titles)\n",
    "texts_inv_idx = create_inverted_index(prep_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titles_inv_idx.get('machine', []))\n",
    "print(texts_inv_idx.get('neural', []))\n",
    "print(prep_titles[2])\n",
    "print(prep_texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. část - Implementace TF-IDF\n",
    "\n",
    "Připravení funkce pro výpočet TF-IDF po příchodu dotazu. Funkce *tf_idf* by měla pracovat s dotazem, jedním invertovaným indexem a s danými dokumenty. Vrátit by měla list obsahující skóre pro každý dokument.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$\n",
    "score(q,d) = TF\\_IDF(q,d) = \\sum\\limits_{w \\in q \\cap d} c(w, q) c(w, d) log(\\frac{M+1}{df(w)})\n",
    "$\n",
    "</center>\n",
    "\n",
    "$q$ ... dotaz<br>\n",
    "$d$ ... dokument<br>\n",
    "$c(w, q)$ ... kolikrát je slovo *w* v dotazu *q*<br>\n",
    "$M$ ... celkový počet dokumentů<br>\n",
    "$df(w)$ ... počet dokumentů, ve kterých se nachází slovo *w*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query: str, inverted_index: defaultdict[list], docs: list[str]) -> list: \n",
    "    M = len(docs)\n",
    "    \n",
    "    # Count the occurrences of each term in the query\n",
    "    query_term_counts = defaultdict(int)\n",
    "    for term in preprocess(query).split():\n",
    "        query_term_counts[term] += 1\n",
    "    # print(query_term_counts)\n",
    "    \n",
    "    # Initialize scores for each document\n",
    "    scores = [0.0] * M\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    for term, term_count in query_term_counts.items():\n",
    "        # print(f\"1. Curr term: {term} ({term_count})\")\n",
    "        # Check if the term is in the inverted index\n",
    "        if term in inverted_index:\n",
    "            # Get the list of documents containing the term\n",
    "            doc_id_list = inverted_index[term]\n",
    "            # print(f\"2. Doc ID list that cointain '{term}': \\n{doc_id_list}\")\n",
    "            # Document frequency\n",
    "            df_w = len(doc_id_list)\n",
    "            # print(f\"3. DF_w = {df_w}\")\n",
    "            # print(f\"{term} - {doc_id_list}\")\n",
    "            \n",
    "            # Calculate the contribution to the score for each document\n",
    "            for doc_id in doc_id_list:\n",
    "                # Count occurrences of the term in the document\n",
    "                doc_term_count = docs[doc_id].split().count(term)\n",
    "                # print(f\"4. doc_id: {doc_id}, count in doc with curr ID: {doc_term_count}. Term: {term}\")\n",
    "                # TF-IDF score for the term in the document\n",
    "                score_contribution = term_count * doc_term_count * math.log((M + 1) / df_w)\n",
    "                # print(f\"5. Score: {score_contribution}\")\n",
    "                # Update the score for the document\n",
    "                # print(f\"6. Curr score for this doc: {scores[doc_id]}\")\n",
    "                scores[doc_id] += score_contribution\n",
    "                # print(f\"  After: {scores[doc_id]}\")\n",
    "\n",
    "                # input(\">>\")\n",
    "                # print(\"=\" * 50)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. část - Použití a testování TF-IDF\n",
    "\n",
    "Nyní lze získat skóre pro titulky nebo text. Následujícím krokem je sjednocení výsledného skóre pro ohodnocení celého dokumentu. V případě dvou hodnot si vystačíme s parametrem $\\alpha$, který nám určuje jakou váhu má titulek a jakou samotný text dokumentu. <br>\n",
    "\n",
    "<center>\n",
    "$\n",
    "score(q,d) = \\alpha \\; TF\\_IDF\\_title(q,d) + (1-\\alpha) \\; TF\\_IDF\\_text(q,d)\n",
    "$\n",
    "</center>\n",
    "\n",
    "Při nastavení parametru $\\alpha$ na hodnotu 0.7 a vyhledávání dotazu \"coursera vs udacity machine learning\" by výsledky měly vypadat následovně:\n",
    "\n",
    "![output](sample_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title score for 276: 18.488162537198818\n",
      "Text score for 276: 85.70941791888\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>coursera vs udacity for machine learn   hacker...</td>\n",
       "      <td>2018 be an exciting time for student of machin...</td>\n",
       "      <td>38.654539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>every single machine learning course on the in...</td>\n",
       "      <td>a year and a half ago I drop out of one of the...</td>\n",
       "      <td>19.560162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>every single machine learning course on the in...</td>\n",
       "      <td>a year and a half ago I drop out of one of the...</td>\n",
       "      <td>19.560162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>every single machine learning course on the in...</td>\n",
       "      <td>a year and a half ago I drop out of one of the...</td>\n",
       "      <td>19.560162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>every single machine learning course on the in...</td>\n",
       "      <td>a year and a half ago I drop out of one of the...</td>\n",
       "      <td>19.560162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>what be the good intelligent chatbot or ai cha...</td>\n",
       "      <td>how do we define the intelligence of a chatbot...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>multiindex locality sensitive hash for fun and...</td>\n",
       "      <td>one way that we deal with this volume of datum...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>multistream rnn concat rnn internal conv rnn l...</td>\n",
       "      <td>for the last two week I have be die to impleme...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>classify website with neural network   knowled...</td>\n",
       "      <td>at datafiniti we have a strong need for conver...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6 trick I learn from the otto kaggle challenge...</td>\n",
       "      <td>here be a few thing I learn from the otto grou...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "276  coursera vs udacity for machine learn   hacker...   \n",
       "143  every single machine learning course on the in...   \n",
       "99   every single machine learning course on the in...   \n",
       "67   every single machine learning course on the in...   \n",
       "19   every single machine learning course on the in...   \n",
       "..                                                 ...   \n",
       "267  what be the good intelligent chatbot or ai cha...   \n",
       "33   multiindex locality sensitive hash for fun and...   \n",
       "286  multistream rnn concat rnn internal conv rnn l...   \n",
       "31   classify website with neural network   knowled...   \n",
       "52   6 trick I learn from the otto kaggle challenge...   \n",
       "\n",
       "                                                  Text      Score  \n",
       "276  2018 be an exciting time for student of machin...  38.654539  \n",
       "143  a year and a half ago I drop out of one of the...  19.560162  \n",
       "99   a year and a half ago I drop out of one of the...  19.560162  \n",
       "67   a year and a half ago I drop out of one of the...  19.560162  \n",
       "19   a year and a half ago I drop out of one of the...  19.560162  \n",
       "..                                                 ...        ...  \n",
       "267  how do we define the intelligence of a chatbot...   0.000000  \n",
       "33   one way that we deal with this volume of datum...   0.000000  \n",
       "286  for the last two week I have be die to impleme...   0.000000  \n",
       "31   at datafiniti we have a strong need for conver...   0.000000  \n",
       "52   here be a few thing I learn from the otto grou...   0.000000  \n",
       "\n",
       "[337 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.7\n",
    "query = 'coursera vs udacity machine learning'\n",
    "titles_scores = np.array(tf_idf(query, titles_inv_idx, prep_titles))\n",
    "# print(len(titles_scores))\n",
    "texts_scores = np.array(tf_idf(query, texts_inv_idx, prep_texts))\n",
    "\n",
    "print(f\"Title score for 276: {titles_scores[276]}\")\n",
    "print(f\"Text score for 276: {texts_scores[276]}\")\n",
    "\n",
    "total_scores = alpha * titles_scores + (1 - alpha) * texts_scores\n",
    "\n",
    "data = {\n",
    "        'Title': prep_titles,\n",
    "        'Text': prep_texts,\n",
    "        'Score': total_scores\n",
    "    }\n",
    "df = pd.DataFrame(data)\n",
    "df_sorted = df.sort_values(by='Score', ascending=False)\n",
    "df_sorted\n",
    "# print(df[\"Title\"][276])\n",
    "# print(df[\"Text\"][276])\n",
    "# print(df[\"Score\"][276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
